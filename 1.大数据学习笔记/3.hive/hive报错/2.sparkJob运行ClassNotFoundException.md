报错内容：

~~~
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job failed with java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.udf.generic.GenericUDF
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Spark job failed during runtime. Please check stacktrace for the root cause.
~~~

报错原因：

~~~
hadoop文件系统 /spark-jars中没有存放hive-exec-3.1.2.jar包，导致sparkJob运行UDF函数是找不到类。

而hive中能注册全局函数，是因为hive/lib中有这个jar包
~~~

解决办法：

~~~
hadoop dfs -put /opt/module/hive/lib/hive-exec-3.1.2.jar /spark-jars
~~~

