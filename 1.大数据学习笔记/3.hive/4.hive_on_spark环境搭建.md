> 不要混淆Hive on Spark 和 Spark on Hive
>
> `Hive on Spark` 核心组件是Hive, 只是把运行的执行引擎替换为了Spark内存计算框架, 提高的程序运行的效率
>
> 其中Hive主要负责数据的存储以及SQL语句的解析。
>
> `Spark on Hive` 核心组件是Spark, 只是把Spark的的数据存储使用Hive以及元数据管理使用Hive, Spark负责SQL的解析并且进行计算

##### 一、hive和spark版本兼容问题

兼容性说明:
	官网下载的 Hive 3.1.2 和 Spark 3.0.0 默认是不兼容的。因为 Hive3.1.2 支持的Spark版本是2.4.5，所以需要我们重新编译Hive3.1.2版本。

编译步骤：
	官网下载Hive3.1.2源码，修改pom文件中引用的Spark版本为3.0.0，如果编译通过，直接打包获取jar包。如果报错，就根据提示，修改相关方法，直到不报错，打包获取jar包。

##### 二、spark环境搭建

1. 环境变量

   /etc/profile.d/my_env.sh

   ~~~sh
   # SPARK_HOME
   export SPARK_HOME=/opt/module/spark
   export PATH=$PATH:$SPARK_HOME/bin
   ~~~

2. 配置文件

   /opt/module/spark-3.0.0-bin-without-hadoop/conf/spark-env.sh

   ~~~sh
   export JAVA_HOME=/opt/module/jdk1.8.0_171
   export HADOOP_HOME=/opt/module/hadoop-3.1.3
   export HADOOP_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop
   export SPARK_DIST_CLASSPATH=$(/opt/module/hadoop-3.1.3/bin/hadoop classpath)
   ~~~

   /opt/module/spark-3.0.0-bin-without-hadoop/conf/spark-defaults.conf

   ~~~
   spark.master                     	yarn
   spark.eventLog.enabled	true
   spark.eventLog.dir		hdfs://hadoop102:8020/spark/log
   spark.serializer                 org.apache.spark.serializer.KryoSerializer
   spark.driver.memory              1g
   spark.executor.memory            1g
   ~~~

##### 三、hive环境搭建

1. 环境变量

   /etc/profile.d/my_env.sh

   ~~~sh
   #HIVE_HOME
   export HIVE_HOME=/opt/module/hive
   export PATH=$PATH:$HIVE_HOME/bin
   ~~~

2. 配置文件

   /opt/module/apache-hive-3.1.2-bin/conf/hive-env.sh

   ~~~sh
   # Set HADOOP_HOME to point to a specific hadoop install directory
     HADOOP_HOME=/opt/module/hadoop-3.1.3
   
   # Hive Configuration Directory can be controlled by:
     export HIVE_CONF_DIR=/opt/module/apache-hive-3.1.2-bin/conf
   
   # Folder containing extra libraries required for hive compilation/execution can be controlled by:
     export HIVE_AUX_JARS_PATH=/opt/module/apache-hive-3.1.2-bin/lib
   ~~~

   /opt/module/apache-hive-3.1.2-bin/conf/hive-site.xml（主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息）

   ~~~xml
   <?xml version="1.0"?>
   <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
   <configuration>
       <!-- jdbc 连接的 URL -->
       <property>
           <name>javax.jdo.option.ConnectionURL</name>
           <value>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true</value>
       </property>
   
       <!-- jdbc 连接的 Driver 2.4.2	配置 Metastore到MySQL使用的driver -->
       <property>
           <name>javax.jdo.option.ConnectionDriverName</name>
           <value>com.mysql.jdbc.Driver</value>
       </property>
   
       <!-- jdbc 连接的 username-->
       <property>
           <name>javax.jdo.option.ConnectionUserName</name>
           <value>root</value>
       </property>
   
       <!-- jdbc 连接的 password -->
       <property>
           <name>javax.jdo.option.ConnectionPassword</name>
           <value>root</value>
       </property>
   </configuration>    
   ~~~

3. 拷贝数据库驱动mysql-connector-java-5.1.37-bin到hive/lib目录下

4. 初始化元数据库（初始化hive-site.xml配置文件指定的数据库）

   ​	**metastore数据库会生成系列元数据目录，如DBS、TBLES、SDS等,存放元数据信息**

##### 四、hive on spark配置

1. hive中创建 spark 配置的文件/opt/module/apache-hive-3.1.2-bin/conf/spark-defaults.conf

2. 上传 Spark 纯净版 jar 包 到hadoop文件系统/spark/jars/中

   ~~~
   说明1：由于Spark3.0.0非纯净版默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。
   
   说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。
   ~~~

3. 配置hive-site.xml 文件

   ~~~xml
   <!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）-->
   <property>
       <name>spark.yarn.jars</name>
       <value>hdfs://hadoop102:8020/spark-jars/*</value>
   </property>
     
   <!--Hive执行引擎-->
   <property>
       <name>hive.execution.engine</name>
       <value>spark</value>
   </property>
   ~~~

4. 测试

   * 启动metastore（**客户端连接metastore服务，metastore再去连接MySQL数据库来存取元数据。有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。**）
   * 进入hive客户端，执行insert into student values(100,'wolf')；可以看到执行引擎已经替换为了Spark内存计算框架