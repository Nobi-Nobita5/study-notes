20230519 NCS一面：

英文：

1.自我介绍

2.static用法

3.线程的阻塞和唤醒

4.描述你最近的一个项目

5.描述Java内存模型

6.hdfs读流程

--------------------------------------------

1.static用法，**修改静态变量值的方法**

> https://www.cnblogs.com/dolphin0520/p/3799052.html
>
> 1）static关键字可修饰变量、方法、类（静态内部类）、代码块，被static修饰，类加载器会优先加载进JVM。
>
> 2）如果想在不创建对象的情况下调用某个方法，就可以将这个方法设置为static。
>
> 3）静态方法中不能使用this，而构造方法中能使用this关键字。this是指调用当前方法的对象，而静态方法不属于任何对象。
>
> 4）可通过this.和类名.获取静态变量，进而修改。

2.Java内存模型

3.Java内存结构

**4.进程之间通信方式**

>  https://blog.csdn.net/GMLGDJ/article/details/124627224
>
> 1）管道
>
> 2）消息队列
>
> 3）共享内存
>
> 4）信号量
>
> 5）信号
>
> 6）socket（BIO、NIO、AIO）

**5.RDD血缘关系，宽依赖、窄依赖、DAG图执行引擎**

**6.spark中的通信框架netty**

7.hdfs的读写流程

8.重入锁的用法，锁用过哪些

9.voltile关键字用法

10.unix和linux区别联系

**11.RPC框架怎么用**

12.公司项目数据量

13.git冲突怎么解决

14.Spark深入理解，分布式理论

15. **happens-before是什么？规则有哪些？**

已挂

---

0529

1. 已刷完OD机试题

---

0530

1. 博客项目聊天室部分总结**（整完后腾讯云上部署下 6.14之前）**
2. 复习Sql题、Sql优化

---

0531

1. 复习Sql题、Sql优化
2. 总结力扣、OD笔试
3. 复习Hive、HiveSql**（这天没搞）**

---

0601

1. 复习Hive、HiveSql（0602面试）
2. 复习Sql题、Sql优化
3. 力扣

---

0602 

根网一面

1. 自我介绍
2. 离职原因
3. 发展方向
4. Sql优化
5. 是否会Java
6. **用过什么定时调度组件**
7. 反问业务内容，面试评价

学习内容

1）hiveSql、mapreduce、spark编程

2）调整集群参数

---

0605

**HiveSql能写就行，不要一直配置,学习为主**

1. 用WordCount（文本数据源）走MapReduce编程流程。

2. Hive on Spark 调优，跟进Sql运行时Spark、MapReduce具体执行流程。配置有问题，写不了HSql。跟进视频资料学习，主要是概念，少实操。

3. 解决对应面试题（优化）

4. Sql、力扣
   
   > 动态规划：LC_91_解码方法,LC_300_最长递增子序列

---

0606

1. 力扣：NumberCombination、回溯+剪枝
2. 复习MVCC、大数据场景题（去重、TopK）

---

0607

1. 力扣：动态规划、复习

   > ```
   > LC_5_最长回文子串，LC_509_斐波那契数
   > ```

2. 100问面经过一遍、基础笔记过一遍、并发集合案例

**拼多多一面：**

1. 你理解的什么叫大数据

2. 为什么要对数据仓库分层（100问43题）

3. 如何判断建设的数据仓库的好坏

4. 你在恒生的数据开发承担了哪些工作（**简历问题**）

5. 数据平台可以给数据开发的同学提供哪些服务

6. map任务和reduce任务数量是越多越好吗

7. mapreduce相比spark为什么慢

   > MapReduce和Apache Spark都是用于大规模数据处理的框架，但是Spark通常比MapReduce快，原因主要有以下几点：
   >
   > 1. **数据存储**：MapReduce对每一次Map和Reduce操作的输入和输出都存储在磁盘上。磁盘I/O操作通常比内存操作要慢得多，因此频繁的磁盘读写可以显著降低性能。相比之下，Spark支持内存计算，**能够把数据缓存到内存中**，这使得Spark在进行迭代运算和交互式数据挖掘时，能够显著地减少磁盘I/O操作，从而大幅度提高了处理速度。
   >
   >    注：Spark 是基于内存的计算，这不是主要原因，因为要对数据做计算，必然得加载到内存，Hadoop 也是如此。真正的原因是 **Spark 支持将需要反复用到的数据给 Cache 到内存中，减少数据加载耗时**，所以 Spark 跑机器学习算法比较在行（需要对数据进行反复迭代）
   >
   > 2. **计算模型**：MapReduce只支持Map和Reduce两种计算模式，而Spark支持更广泛的计算模式，包括Map、Reduce、Join、Filter等。这使得Spark能够更好地优化计算过程，并且可以对多个操作进行管道处理（pipeline），进一步减少了数据传输和磁盘I/O。
   >
   > 3. **延迟**：MapReduce的延迟较高，每一次Map或Reduce操作都需要在Hadoop系统中启动一个新的JVM进程，而这个过程的开销相当大。而Spark可以复用已有的Executor，避免了频繁的JVM启动，降低了延迟。
   >
   > 总的来说，由于Spark的设计和计算模型更适合现代的大数据处理需求，因此在大多数情况下，Spark的性能都会优于MapReduce。然而，这并不是说Spark在所有情况下都比MapReduce更好，例如在处理极大规模的数据集或者非常复杂的数据管道时，MapReduce可能会更有优势。

8. 双流join如何实现的

9. sql题目：输出表中num字段最长的连续递增序列

   表t如下：

   ~~~
   id num
   1		2
   2		3
   3		4
   4		3
   5		4
   6		5
   7		6
   8		4
   9		5
   应该输出：3 4 5 6
   ~~~

10. **jdk1.8的新特性**

11. **olap除了hive、ES，还了解哪些**

12. **JVM如何优化，如何排查定位无限GC的问题**

13. **相对于spark，flink在流式处理上，怎么提供了可靠性和稳定性，有哪些优点**

    > Apache Flink 是一个大规模数据处理引擎，对于批处理和流处理都提供了强大的支持。相比于 Apache Spark，它在流式处理上有以下的优势：
    >
    > 1. **真正的流处理**：Spark 在处理流数据时，采用的是**微批处理模型**，而 Flink 采用的是**事件驱动的模型**。这就使得 Flink 可以在事件发生后立即进行处理，减少了延迟。
    > 2. **事件时间和处理时间**：Flink 支持**事件时间（Event Time）和处理时间（Processing Time）**两种时间语义，且可以非常方便的在两者之间进行切换。这使得 Flink 在处理有延迟的数据、乱序数据或者需要处理时间窗口的任务时具有极大的优势。
    > 3. **窗口和触发机制**：Flink 提供了丰富的窗口操作以及自定义的触发机制，比如滑动窗口、滚动窗口等等。这可以更灵活地处理各种复杂的流数据处理场景。
    > 4. **容错和恢复机制**：**Flink 提供了一种名为"异步快照"的容错机制**，这使得在节点失败时，系统可以从之前的状态进行恢复，保证了处理过程的正确性。同时，由于这种机制是异步进行的，对于处理性能的影响也非常小。
    > 5. **通过反压机制确保稳定性**：在流处理中，常常会遇到上游数据产生速度大于下游处理速度的问题，这可能会导致系统的崩溃。而 Flink 通过反压机制来防止这种情况的发生，当下游无法处理更多的数据时，会向上游发送反压信号，让其降低数据产生的速度，这在一定程度上保证了系统的稳定性。
    > 6. **CEP（复杂事件处理）**：Flink 提供了一种名为 CEP 的复杂事件处理机制，可以方便地定义和处理复杂的事件模式。
    > 7. **内存管理**：Flink 有自己的内存管理机制，**避免了频繁的 GC**，从而保证了更好的性能。
    >
    > 以上就是 Flink 相对于 Spark 在流式处理上的一些优点，但这并不是说 Flink 就完全优于 Spark，两者各有所长，具体使用哪个，还需要根据实际的应用场景和需求来决定。

14. 博客项目有多少数据量，spark on yarn怎么用的

    > 1. 恒生资产负债表1亿条数据，每条json数据都用String存储每个字段，如果一条20个字段占248个字节，那一亿条就是23G左右。
    >
    > 2. 博客项目，每批次生成1千万条日志数据，发送到kafka。每条约占408字节，一千万条就是3.8G左右。
    >    - "actions": 约60字节（假设数组长度为1）
    >    - "common": 约160字节
    >    - "displays": 约100字节（假设数组长度为2）
    >    - "page": 约80字节
    >    - "ts": 8字节

15. 手上其他offer

16. 工作强度10 11 6能接受吗

17. 反问

18. https://www.kanzhun.com/firm/interview/detail/1nV_3ti5Elc~.html

---

  0614

1.  微众一面
    1. JVM内存结构
    2. GC为什么分代
    2. JVM调优经验
    3. **HDFS块大小为什么64MB(或128MB或256MB)是最优选择**
    3. NameNode相关、副本机制等
    4. 什么是MapReduce
    5. 什么是数据倾斜
    6. 什么是map join
    6. 项目负责哪些？建模方法
    7. **zk选主机制**，可以扯以下分布式一致性协议
    8. HDFS HA原理
    9. **Hive Sql优化方法**
    10. **Sql优化方法,怎么优化的**
    11. 场景题：一张表，查询每个产品有多少数据量，sql怎么写。
    
    ---
    
    博客搭建

---

0616

飞猪一面

1. 两个sql题目
2. 工作中如何进行的**模型优化**？
3. 需要计算用户求存率，如何设计数据仓库？

---

0620

微众二面

1. 数据仓库模型设计案例（先说设计方法步骤）
1. 星型模型和雪花模型的区别
2. **Yarn调度机制**
3. MapReduce流程
4. Sql、Hive Sql优化方法
5. 工作中遇到的最大的挑战（说Hive 优化，解决数据倾斜；说项目并行推进；）

---

0623

如何进行数据仓库建模的

---

0624

1. 技术面试点复习，大数据、Java

2. 如何用Java实现一亿条数据的excel导出

   > 要使用Java并发编程高效地从Oracle数据库导出一亿条数据到Excel，我们可以使用线程池来并行处理数据库查询，并使用Apache POI的SXSSFWorkbook来流式地写入数据到Excel。这是一个较为复杂的任务，所以下面的代码示例只是一个简化的版本，仅供参考：
   >
   > ```java
   > import org.apache.poi.ss.usermodel.*;
   > import org.apache.poi.xssf.streaming.SXSSFWorkbook;
   > 
   > import java.io.FileOutputStream;
   > import java.sql.Connection;
   > import java.sql.DriverManager;
   > import java.sql.PreparedStatement;
   > import java.sql.ResultSet;
   > import java.util.concurrent.ExecutorService;
   > import java.util.concurrent.Executors;
   > import java.util.concurrent.atomic.AtomicInteger;
   > 
   > public class ConcurrentLargeExcelExporter {
   > 
   >     private static final String JDBC_URL = "jdbc:oracle:thin:@your-host:your-port:your-db";
   >     private static final String JDBC_USER = "username";
   >     private static final String JDBC_PASS = "password";
   > 
   >     private static final int PAGE_SIZE = 10000;
   >     private static final int TOTAL_ROWS = 100_000_000;
   > 
   >     public static void main(String[] args) throws Exception {
   >         SXSSFWorkbook workbook = new SXSSFWorkbook(100); // keep 100 rows in memory, exceeding rows will be flushed to disk
   >         Sheet sheet = workbook.createSheet("Large Data");
   > 
   >         // Use thread pool
   >         ExecutorService executor = Executors.newFixedThreadPool(4); // for example, 4 threads
   > 
   >         AtomicInteger currentRow = new AtomicInteger();
   > 
   >         // Submit tasks to thread pool for parallel processing
   >         int totalPages = TOTAL_ROWS / PAGE_SIZE;
   >         for (int pageIndex = 0; pageIndex <= totalPages; pageIndex++) {
   >             final int page = pageIndex;
   >             executor.submit(() -> {
   >                 try {
   >                     // Fetch data from database page by page
   >                     fetchAndWriteData(sheet, currentRow, page * PAGE_SIZE, PAGE_SIZE);
   >                 } catch (Exception e) {
   >                     e.printStackTrace();
   >                 }
   >             });
   >         }
   > 
   >         // Shutdown the executor
   >         executor.shutdown();
   >         while (!executor.isTerminated()) {
   >             // Wait for all threads to finish
   >         }
   > 
   >         // Write the output to a file
   >         FileOutputStream out = new FileOutputStream("large_data.xlsx");
   >         workbook.write(out);
   >         out.close();
   > 
   >         // Dispose of temporary files backing this workbook on disk
   >         workbook.dispose();
   >     }
   > 
   >     private static void fetchAndWriteData(Sheet sheet, AtomicInteger currentRow, int offset, int limit) throws Exception {
   >         try (Connection conn = DriverManager.getConnection(JDBC_URL, JDBC_USER, JDBC_PASS)) {
   >             String sql = "SELECT * FROM YOUR_TABLE FETCH FIRST ? ROWS ONLY OFFSET ? ROWS"; // Modify this query according to your table schema
   >             try (PreparedStatement ps = conn.prepareStatement(sql)) {
   >                 ps.setInt(1, limit);
   >                 ps.setInt(2, offset);
   >                 try (ResultSet rs = ps.executeQuery()) {
   >                     while (rs.next()) {
   >                         Row row = sheet.createRow(currentRow.getAndIncrement());
   >                         for (int i = 1; i <= rs.getMetaData().getColumnCount(); i++) {
   >                             Cell cell = row.createCell(i - 1);
   >                             cell.setCellValue(rs.getString(i));
   >                         }
   >                     }
   >                 }
   >             }
   >         }
   >     }
   > }
   > ```
   >
   > 需要注意的是：
   >
   > - 使用了`ExecutorService`来**并发处理数据的读取和写入**。
   > - 数据库查询使用了**分页来减少内存使用**。
   > - 为简化示例，错误处理和优化被保持在最低。实际应用中，你需要加强错误处理，**调优数据库连接**和查询，以及优
   >
   > 化Excel写入性能。
   > - 请根据实际的数据库表结构和环境修改代码。这只是一个示例，用于说明如何使用Java并发编程和Apache POI来高效地导出大量数据到Excel。

---

0625

结合项目业务案例，模拟面试。

一、公司项目

（要涉及到离线数仓建模，Sql优化，HiveSql优化，数据倾斜处理，UDF函数）

1. 项目介绍：

​		1）业务背景，技术架构

​		2）项目中承担的工作内容（2/3的HiveSql和MapReduce程序进行统计分析、基于Oracle数据仓库开发、也会参与数据建模，1/3的后台开发）

2. 数据建模
3. Sql优化
4. HiveSql优化、高性能UDF函数
5. 数据倾斜处理
6. 12家项目竣工推进

二、博客项目

（要涉及到Spark Streaming，Kafka，数据倾斜处理，流式关联）

1. 项目介绍：

   1）业务背景，技术架构

   2）实现的功能

2. Spark Streaming消费Kafka的问题

3. 流式关联问题

4. 数据倾斜问题

---

0701

1. 完成平台开发工程师简历，并投递

---

0703

1. 算法3道：

   ```
   LC_17_电话号码的字母组合，LC_11_盛最多水的容器，LC_42_接雨水
   ```
