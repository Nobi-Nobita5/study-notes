汇总：https://blog.csdn.net/zhou4411781/article/details/120049657

### 一、Hive On MR优化的思路和方法

#### 1. 使用适当的存储格式，开启压缩

​	例如，Parquet 和 ORC 是列式存储格式，通常更适合于分析型查询。

​	使用压缩可以减少存储空间和数据传输的开销。常见的压缩格式包括 Snappy, Gzip 和 Zlib。

#### 2. **优化表**：

​	1）MapJoin：小表直接加载进内存，在 Map 进行 Join 操 作，不用进行Reduce

​	2）通过合理地对数据进行分区和分桶，可以减少查询时需要扫描的数据量。

​	3）Group By：部分聚合操作在Map端进行，避免数据倾斜。通过如下参数实现

​								set hive.groupby.skewindata = true

#### 3. **优化HiveSql语句**：

​	简化和优化查询语句，比如使用合适的JOIN策略，避免使用笛卡尔积，合理使用子查询等。

#### 4. **调整内存和并行度**：

​	根据系统资源配置合理的内存设置，并调整 map 和 reduce 的并行度。

​	通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。注意，在共享集群中，需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。

#### 5. 严格模式

​	Hive 可以通过设置防止一些危险操作

#### 6. 处理数据倾斜

​	把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，可以使用`DISTRIBUTE BY`和`SORT BY`语句来重新分布数据，以避免某些reduce任务过载。

#### 7. JVM重用

​	默认情况下，每个 Task 任务都需要启动一个 JVM 来运行，如果 Task 任务计算的数据量很小，我们可以让同一个 Job 的多个 Task 运行在一个 JVM 中，不必为每个 Task 都开启一个 JVM。主要针对小文件。

​	开启 uber 模式，在 mapred-site.xml 中添加如下配置：

~~~
<!-- 开启 uber 模式，默认关闭 -->
<property>
	<name>mapreduce.job.ubertask.enable</name>
	<value>true</value>
</property>
~~~

#### 8. 通过UDF函数：优化计算密集任务，实现自定义聚合等

让我们来看一个简单的例子，其中使用Hive UDF来优化查询性能，并使用UDAF来进行自定义聚合。

假设我们有一个销售数据表，其中包含产品ID、销售日期和销售额。

**优化查询性能 - UDF**

假设我们需要计算销售额的增长率。使用SQL计算增长率可能会涉及复杂的子查询和连接，这在大数据集上可能非常低效。我们可以创建一个UDF来计算增长率：

```java
package myudfs;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.DoubleWritable;

public final class GrowthRate extends UDF {
    public DoubleWritable evaluate(final DoubleWritable current, final DoubleWritable previous) {
        if (current == null || previous == null || previous.get() == 0) {
            return null;
        }
        double growthRate = (current.get() - previous.get()) / previous.get();
        return new DoubleWritable(growthRate);
    }
}
```

然后，我们可以在Hive查询中使用这个UDF：

```sql
ADD JAR /path/to/myudfs.jar;
CREATE TEMPORARY FUNCTION growth_rate AS 'myudfs.GrowthRate';

SELECT product_id, sales_date, growth_rate(sales_amount, LAG(sales_amount) OVER (PARTITION BY product_id ORDER BY sales_date))
FROM sales_data;
```

**自定义聚合 - UDAF**

假设我们需要计算每个产品的销售额的中位数。Hive没有内置的中位数聚合函数，所以我们可以通过创建一个UDAF来完成这个任务：

```java
package myudafs;
import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
import java.util.ArrayList;
import java.util.Collections;

public class MedianSales extends UDAF {
    public static class MedianSalesEvaluator implements UDAFEvaluator {
        private ArrayList<Double> salesList;

        public MedianSalesEvaluator() {
            super();
            salesList = new ArrayList<Double>();
        }

        public boolean iterate(Double value) {
            if (value == null) {
                return true;
            }
            salesList.add(value);
            return true;
        }

        public ArrayList<Double> terminatePartial() {
            return salesList;
        }

        public boolean merge(ArrayList<Double> other) {
            if (other == null) {
                return true;
            }
            salesList.addAll(other);
            return true;
        }

        public Double terminate() {
            Collections.sort(salesList);
            int size = salesList.size();
            if (size % 2 == 0) {
                return (salesList.get(size/2 - 1) + salesList.get(size/2)) / 2;
            } else {
                return salesList.get(size/2);
            }
        }
    }
}
```

> 这个中位数聚合UDF是使用Hive的UDAF（User-Defined Aggregate Function）来实现的。在这个示例中，我们创建了一个名为`MedianSales`的类，它继承自`UDAF`。为了实现UDAF，我们需要提供一个内部类（在这个例子中是`MedianSalesEvaluator`），该内部类实现了`UDAFEvaluator`接口。`UDAFEvaluator`接口要求我们实现几个方法来处理聚合逻辑。
>
> 以下是每个方法的简单解释：
>
> 1. **iterate**: 这个方法用于处理新的数据点。在这个示例中，它将新的销售额值添加到一个ArrayList（salesList）中。
>
> 2. **terminatePartial**: 当Hive决定将多个部分聚合结果合并时，它使用这个方法来得到部分聚合结果。在我们的例子中，这个方法简单地返回包含部分聚合数据的ArrayList。
>
> 3. **merge**: 此方法用于合并来自不同部分的聚合结果。在这个示例中，它将另一个ArrayList合并到主ArrayList中。
>
> 4. **terminate**: 这是在完成所有聚合后调用的方法，以生成最终聚合结果。在这个示例中，我们对包含所有销售额值的ArrayList进行排序，然后找出中位数。如果列表的大小是偶数，则返回中间两个值的平均值。如果列表大小是奇数，则返回中间值。
>
> 通过这种方式，`MedianSales` UDAF能够处理大量的数据，并且可以并行地在多个reducers上执行。当所有reducers完成处理后，它们的结果被合并以计算最终的中位数。
>
> 值得注意的是，在处理非常大的数据集时，此方法可能会遇到内存问题，因为它将所有销售额值存储在内存中的一个列表中。在生产环境中，可能需要考虑更高效的算法来计算中位数，例如使用近似算法。

然后，我们可以在Hive查询中使用这个UDAF：

```sql
ADD JAR /path/to/myudafs.jar;
CREATE TEMPORARY FUNCTION median_sales AS 'myudafs.MedianSales';

SELECT product_id, median_sales(sales_amount)
FROM sales_data
GROUP BY product_id;
```

注意，创建和使用UDF和UDAF需要对Java编程和Hive有一定了解，并且需要谨慎测试以确保正确性和性能。

#### 8. 合理设置 Map 及 Reduce 数

- [（1）复杂文件增加Map数](https://blog.csdn.net/zhou4411781/article/details/120049657#1Map_284)
- [（2）小文件进行合并](https://blog.csdn.net/zhou4411781/article/details/120049657#2_291)
- [（3）合理设置Reduce数](https://blog.csdn.net/zhou4411781/article/details/120049657#3Reduce_323)

~~~~
一、    控制hive任务中的map数: 

1.    通常情况下，作业会通过input的目录产生一个或者多个map任务。 
主要的决定因素有： input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)；

2.    举例： (基于FileInputFormat 切片规则)
a)    假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数
b)    假设input目录下有3个文件a,b,c,大小分别为10m，20m，160m，那么hadoop会分隔成4个块（10m,20m,128m,32m）,从而产生4个map数
即，如果文件大于块大小(128m)的1.1倍,那么会拆分，如果小于块大小，则把该文件当成一个块。

3.    是不是map数越多越好？ 
答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成，
而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。
而且，同时可执行的map数是受限的。

4.    是不是保证每个map处理接近128m的文件块，就高枕无忧了？ 
答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，
如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。
~~~~

* 如何合并小文件，减少map数？ 

  > 假设一个SQL任务：
  >       Select count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’;
  >       该任务的inputdir /group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04
  >       共有194个文件，其中很多是远远小于128m的小文件，总大小9G，正常执行会用194个map任务。
  >       Map总共消耗的计算资源： SLOTS_MILLIS_MAPS= 623,020
  >
  > ​      我通过以下方法来在map执行前合并小文件，减少map数：
  > ​      set mapred.max.split.size=100000000;
  > ​             set mapred.min.split.size.per.node=100000000;
  > ​             set mapred.min.split.size.per.rack=100000000;
  > ​             set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
  > ​           再执行上面的语句，用了74个map任务，map消耗的计算资源：SLOTS_MILLIS_MAPS= 333,500
  > ​      对于这个简单SQL任务，执行时间上可能差不多，但节省了一半的计算资源。
  > ​      大概解释一下，100000000表示100M, **set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;这个参数表示执行前进行小文件合并**，
  > ​      前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），
  > ​      进行合并,最终生成了74个块。

* 如何适当的增加map数？  

  > ​      当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。
  > ​      假设有这样一个任务：
  > ​      Select data_desc,
  > ​          count(1),
  > ​          count(distinct id),
  > ​          sum(case when …),
  > ​          sum(case when ...),
  > ​          sum(…)
  > ​     from a group by data_desc
  > ​            如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，
  > ​            这样就可以用多个map任务去完成。
  > ​            set mapred.reduce.tasks=10;
  > ​            create table a_1 as 
  > ​            select * from a 
  > ​            distribute by rand(123); 
  > ​            
  > ​            这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。
  > ​            每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多。
  >
  >   看上去，貌似这两种有些矛盾，一个是要合并小文件，一个是要把大文件拆成小文件，这点正是重点需要关注的地方，
  >   根据实际情况，控制map数量需要遵循两个原则：使大数据量利用合适的map数；使单个map任务处理合适的数据量；


二、    控制hive任务的reduce数： 

> 1. Hive自己如何确定reduce数： 
>    reduce个数的设定极大影响任务执行效率，不指定reduce个数的情况下，Hive会计算确定一个reduce个数，基于以下两个设定：
>    hive.exec.reducers.bytes.per.reducer（每个reduce任务处理的数据量，默认为1000^3=1G） 
>    hive.exec.reducers.max（每个任务最大的reduce数，默认为999）
>    计算reducer数的公式很简单N=min(参数2，总输入数据量/参数1)
>    即，如果reduce的输入（map的输出）总大小不超过1G,那么只会有一个reduce任务；
>    如：select pt,count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04' group by pt; 
>          /group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04 总大小为9G多，因此这句有10个reduce
>
> 2.    调整reduce个数方法一： 
>       调整hive.exec.reducers.bytes.per.reducer参数的值；
>       set hive.exec.reducers.bytes.per.reducer=500000000; （500M）
>       select pt,count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04' group by pt; 这次有20个reduce
> 3.    调整reduce个数方法二； 直接设置任务的reduce个数
>       set mapred.reduce.tasks = 15;
>       select pt,count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04' group by pt;这次有15个reduce
> 4.    reduce个数并不是越多越好； 
>       同map一样，启动和初始化reduce也会消耗时间和资源；
>       另外，有多少个reduce,就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；
> 5.    什么情况下只有一个reduce； 
>       很多时候你会发现任务中不管数据量多大，不管你有没有设置调整reduce个数的参数，任务中一直都只有一个reduce任务；
>       其实只有一个reduce任务的情况，除了数据量小于hive.exec.reducers.bytes.per.reducer参数值的情况外，还有以下原因：
>       a)    没有group by的汇总，比如把select pt,count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04' group by pt; 写成 select count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04';
>       这点非常常见，希望大家尽量改写。
>       b)    用了Order by(全局有序)
>       c)    有笛卡尔积
>       通常这些情况下，除了找办法来变通和避免，我暂时没有什么好的办法，**因为这些操作都是全局的**，所以hadoop不得不用一个reduce去完成；
>       同样的，在设置reduce个数的时候也需要考虑这两个原则：使大数据量利用合适的reduce数；使单个reduce任务处理合适的数据量；

什么是笛卡尔积：

* 假设身高集合为A={a1,a2,a3,a4……an}，体重集合为B = {b1,b2,b3,b4,b5……}，那么（笛卡尔积）A X B = {<a1,b1>, <a1,b2> ,<a1, b3>,……<a1,bn>,<a2,b1>,<a2,b2>……<an,bn> }

  具体特征是a永远在前面，b永远在后面，因为是A X B，笛卡尔积一共有n X n 个元素，因为A,B数组的元素个数为n,n

### 二、如何处理Mapreduce数据倾斜

总结：

1）使用合适的分区函数

2）使用 Combiner 在 Map 端进行局部聚合，以减少传输到 Reducer 的数据量。

3）二次排序。set hive.groupby.skewindata = true

4）组合键、或给倾斜的键添加随机前缀或后缀

1. **使用合适的分区函数**：自定义分区函数以更均匀地分布数据到不同的 Reducer。避免使用默认的哈希分区，因为它可能导致数据倾斜。
2. **组合键（Composite Keys）**：对于 join 操作，使用组合键来将相关数据分布到同一个 Reducer。组合键包含**原始键和附加属性**，这有助于分散数据。
3. **使用随机前缀或后缀**：为倾斜的键添加随机前缀或后缀，以将数据分散到多个 Reducer。在 Reducer 端，你需要去除这些前缀或后缀并进行后续处理。
4. **分离倾斜的键**：识别并分离出倾斜的键，并将它们单独处理。然后将结果与其他数据合并。
5. **在 Map 端预聚合**：对于聚合任务，使用 Combiner 在 Map 端进行局部聚合，以减少传输到 Reducer 的数据量。
6. **使用辅助排序**：在 Map 阶段使用辅助排序来预聚合数据，从而减少 Reducer 上的计算量。
7. **动态调整 Reducer 数量**：根据输入数据的大小和分布动态调整 Reducer 的数量。
8. **使用二次排序**：set hive.groupby.skewindata = true。在第一次 MapReduce 作业中，将数据均匀分布到 Reducer，并在第二次作业中进行排序和聚合。
9. **使用采样**：对输入数据进行采样以估计数据分布，并基于此调整分区策略。