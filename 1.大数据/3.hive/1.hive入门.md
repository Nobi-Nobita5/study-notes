https://blog.csdn.net/weixin_45307196/article/details/108055803?ops_request_misc=&request_id=&biz_id=102&utm_term=hive%E8%AF%A6%E7%BB%86&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-108055803.nonecase&spm=1018.2226.3001.4187

一、 hive不是数据库

hive的本质其实就是hadoop的一个客户端，hive底层不存储任何数据，hive表的数据存在hdfs上，hive表的元数据存在关系型数据库中

默认是derby，我们不一般不用默认的derby来存，一般都会修改为mysql。

**元数据：描述数据的数据**

Hive其实就是将用户写的HQL，给翻译成对应的mr模板，然后执行这些mr程序

二、因为底层执行引擎还是mr，所以延迟较高，不能像关系型数据库那样，立马返回结果

​		并且**底层存储是hdfs，不支持随机写，只能追加**，所以hive不支持行级别的更新和删除（delete 和 update）

三、数据类型

1. 基本数据类型
   * TINYINT、SMALINT、INT、BIGINT、BOOLEAN、FLOAT、DOUBLE、STRING、TIMESTAMP、BINARY

2. 集合数据类型

   * STRUCT：和c语言中的struct类似
   * MAP
   * ARRAY

3. 利用hive数据类型建表测试

   * 建表

     ~~~sql
     create table person(
     name string,
     friends array<string>,
     children map<string,int>,
     address struct<street:string,city:string,email:int>
     )
     row format delimited fields terminated by ','
     collection items terminated by '_'
     map keys terminated by ':'
     lines terminated by '\n';
     ~~~

   * 加载数据

     ~~~sql
     load data local inpath '/opt/module/hive/resources/txt/emp.txt' into table person;
     LOAD DATA  INPATH "hdfs://hadoop102:8020/hive/data_tmp/resources/txt/emp.txt" OVERWRITE INTO TABLE emp;
     ~~~
   
   * 查询数据
   
     ~~~sql
     select * from person;
     ~~~
   
   * 查出来songsong 这个人的姓名，第一个朋友，孩子xiaoxiao song的年龄，和他的邮编
   
     ~~~sql
     select name, friends[0],children['xiaoxiao song'],address.email from person where name = "songsong";
     ~~~

四、DDL数据定义语言

1. 库的DDL

   ~~~sql
   create database if exists db_hive  --指定数据库名称
   comment "this is my first hive"  --指定数据库描述
   location '/db_hive2'  --指定创建的数据库在hdfs上存储的路径
   with dbproperties (name = "xionghx");  -指定库的一些属性
   ~~~

2. 查询数据库

   ~~~sql
   show databse;
   ~~~

3. ......

4. 表的DDL

   ~~~~sql
   CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name          --指定表名  【external 外部表/内部表】
   [(col_name data_type [COMMENT col_comment], ...)]           --指定表的列名，列类型 【列描述】
   [COMMENT table_comment]                                     --指定表的描述
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  --指定分区表的分区字段（分区字段可以是多个）
   [CLUSTERED BY (col_name, col_name, ...)     --指定分桶表的分桶字段  
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]  --指定分桶表桶内排序字段   指定分桶的个数
   [ROW FORMAT DELIMITED      --以下指定hive表在hdfs上存储的原始数据的格式
    [FIELDS TERMINATED BY char]     --每行数据中字段的分隔符  ascII码表的第一个字符  ^A 
    [COLLECTION ITEMS TERMINATED BY char]   --集合元素分隔符  ascII码表的第二个字符  ^B
    [MAP KEYS TERMINATED BY char]  --map集合中 key 和 value 的分隔符    ascII码表的第三个字符  ^C
    [LINES TERMINATED BY char]      --每行数据的分隔符     默认值：'\n'
   ]  
   [STORED AS file_format]         --指定hive的数据在hdfs上存储的格式
   [LOCATION hdfs_path]            --指定hive数据在hdfs上存储的路径  默认值 /user/hive/warehouse/数据库名
   [TBLPROPERTIES (property_name=property_value, ...)]    --指定表的属性
   [AS select_statement]    --按照as后面的查询语句的结果来创建表，复制表结构以及表数据
   [LIKE table_name]     --按照like后面的表结构来创建表，只复制表结构，不复制表数据
   ~~~~
   
   ~~~sql
   --内部表和外部表
   管理表（内部表）：hive掌控者这个数据的生命周期，如果删除一个管理表，hdfs上存储的数据也跟着一起删除。所以一般我们创建管理表时，一般不会再location表的存储路径，就默认放在/user/hive/warehouse下
   
   外部表：hive不完全掌控外部表的数据的生命周期，删除外部表，只删除hive表的元数据，不会删除掉hdfs上存储的数据
   
   一般外部表都是先有的hdfs上的数据，然后我们创建一个外部表，手动指定这个外部表的存储路径
   ~~~
   
   

六、查询DQL

4. 排序
   * 全局排序order by
   
     ~~~sql
     -- Order By可以指定多个排序键，还可以指定升序或降序排序
     ~~~
   
   * distribute by + sort by == Cluster by
   
     如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这可以使用 DISTRIBUTE BY 字句。需要注意的是，DISTRIBUTE BY 虽然能把具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。如果想让 Reducer 上的数据是有序的，可以结合 `SORT BY` 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。
   
     ~~~sql
     -- 将数据按照部门分发到对应的 Reducer 上处理
     -- Sort by 只能指定一个排序键、只能按照升序排序
     SELECT empno, deptno, sal FROM emp DISTRIBUTE BY deptno SORT BY deptno ASC;
     ~~~
   
   * Cluster by
   
     如果 `SORT BY` 和 `DISTRIBUTE BY` 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 `CLUSTER BY` 进行替换，同时 `CLUSTER BY` 可以保证数据在全局是有序的。

七、分区表、分桶表

1. 分区表

   hive存在的问题：Hive 3.0 移除了索引功能，每次查询的时候，都会暴力扫描每张表。

   分区表的目的就是分目录，按照业务需求，把数据分成多个目录存储，查询的时候就可以通过where指定分区

   ~~~sql
   --创建分区语法
   create table dept_partition(
   deptno int, dname string, loc string
   )
   partitioned by (day string)
   row format delimited fields terminated by '\t';
   分区字段属于分区表的一个伪列，数据里面并没有记录这列的值，分区字段的值体现在分区目录名上面。
   
   --往分区表里load数据，要指定分区名
   load data local inpath '/opt/module/hive/datas/dept_20200401.log' into table dept_partition partition(day='20200401');
   
   --分区表查询数据
   select * from dept_partition where day = '20200401';
   
   --删除分区
   alter table dept_partition drop partition (day='__HIVE_DEFAULT_PARTITION__');
   --删除多个分区  注意：多个分区间必须有逗号，没有会报错
   alter table dept_partition drop partition(day='20200405'),partition(day='20200406');
   
   --增加分区
    alter table dept_partition add partition(day='20200404') ;
    --增加多个分区  注意：多个分区间不能逗号，有会报错
    alter table dept_partition add partition(day='20200405') partition(day='20200406');
   ~~~

2. 分桶表

   ~~~sql
   2.1 简介
     分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。
     分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。
     
   2.2 创建分桶表
   在 Hive 中，我们可以通过 CLUSTERED BY 指定分桶列，并通过 SORTED BY 指定桶中数据的排序参考列。下面为分桶表建表语句示例：
     CREATE EXTERNAL TABLE emp_bucket(
       empno INT,
       ename STRING,
       job STRING,
       mgr INT,
       hiredate TIMESTAMP,
       sal DECIMAL(7,2),
       comm DECIMAL(7,2),
       deptno INT)
       CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS  --按照员工编号散列到四个 bucket 中
       ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"
       LOCATION '/hive/emp_bucket';
       
   2.3 加载数据到分桶表
   这里直接使用 Load 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。
   
   load data只是把文件上传到 表所在的HDFS目录下。并没有做其他操作。这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：
   
    * 设置强制分桶
    set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步
    * CTAS导入数据
    INSERT INTO TABLE emp_bucket SELECT *  FROM emp;  --这里的 emp 表就是一张普通的雇员表
   ~~~

   **八、索引**
   
   2.1 简介
   
   Hive 在 0.7.0 引入了索引的功能，索引的设计目标是提高表某些列的查询速度。如果没有索引，带有谓词的查询（如'WHERE table1.column = 10'）会加载整个表或分区并处理所有行。但是如果 column 存在索引，则只需要加载和处理文件的一部分。
   
   2.2 索引原理
   
   在指定列上建立索引，会产生一张索引表（表结构如下），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。**在查询涉及到索引字段时，首先到索引表查找索引列值对应的 HDFS 文件路径及偏移量**，这样就避免了全表扫描。
   
   ~~~
   +--------------+----------------+----------+--+
   |   col_name   |   data_type    | comment     |
   +--------------+----------------+----------+--+
   | empno        | int            |  建立索引的列  |   
   | _bucketname  | string         |  HDFS 文件路径  |
   | _offsets     | array<bigint>  |  偏移量       |
   +--------------+----------------+----------+--+
   ~~~
   
   3、索引的缺陷
   
   索引表最主要的一个缺陷在于：**索引表无法自动 rebuild**，这也就意味着如果表中有数据新增或删除，则必须手动 rebuild，重新执行 MapReduce 作业，生成索引表数据。
   
   同时按照[官方文档](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Indexing) 的说明，**Hive 会从 3.0 开始移除索引功能**，主要基于以下两个原因：
   
   - 具有自动重写的物化视图 (Materialized View) 可以产生与索引相似的效果（Hive 2.3.0 增加了对物化视图的支持，在 3.0 之后正式引入）。
   - 使用列式存储文件格式（Parquet，ORC）进行存储时，这些格式支持选择性扫描，可以跳过不需要的文件或块。
   
   > ORC 内置的索引功能可以参阅这篇文章：[Hive 性能优化之 ORC 索引–Row Group Index vs Bloom Filter Index](http://lxw1234.com/archives/2016/04/632.htm)

