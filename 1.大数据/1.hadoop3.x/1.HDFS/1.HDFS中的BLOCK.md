## *HDFS设置BLOCK的目的*

在HDFS里面，data node上的"块"（Block）是HDFS文件的物理分区单位，默认大小是128MB（在Hadoop 2.x版本，而在Hadoop 1.x版本中默认是64MB）

问题: 为什么64MB(或128MB或256MB)是最优选择？

#### 1.为什么不能远少于64MB(或128MB或256MB) （普通文件系统的数据块大小一般为4KB）减少硬盘寻道时间(disk seek time)

1. 减少硬盘寻道时间

   > HDFS设计前提是支持大容量的流式数据操作，所以即使是一般的数据读写操作，涉及到的数据量都是比较大的。假如数据块设置过少，那需要读取的数据块就比较多，
   >
   > 由于数据块在硬盘上非连续存储，普通硬盘因为需要移动磁头，所以随机寻址较慢，读越多的数据块就增大了总的硬盘寻道时间。当硬盘寻道时间比io时间还要长的多时，那么硬盘寻道时间就成了系统的一个瓶颈。合适的块大小有助于减少硬盘寻道时间，提高系统吞吐量。

2. 减少Namenode内存消耗

   > 对于HDFS，他只有一个Namenode节点，他的内存相对于Datanode来说，是极其有限的。然而，namenode需要在其内存FSImage文件中中记录在Datanode中的数据块信息，假如数据块大小设置过少，而需要维护的数据块信息就会过多，那Namenode的内存可能就会伤不起了。

#### 2.为什么不能远大于64MB(或128MB或256MB)？

> 这里主要从上层的MapReduce框架来讨论
>
> Map崩溃问题：系统需要重新启动，启动过程需要重新加载数据，数据块越大，数据加载时间越长，系统恢复过程越长
>
> 监管时间问题：主节点监管其他节点的情况，每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。对于这个“预设的时间间隔”，这是从数据块的角度大概估算的。假如是对于64MB的数据块，我可以假设你10分钟之内无论如何也能解决了吧，超过10分钟也没反应，那就是死了。可对于640MB或是1G以上的数据，我应该要估算个多长的时间内？估算的时间短了，那就误判死亡了，分分钟更坏的情况是所有节点都会被判死亡。估算的时间长了，那等待的时间就过长了。所以对于过大的数据块，这个“预设的时间间隔”不好估算。
>
> 问题分解问题：数据量大小是问题解决的复杂度是成线性关系的。对于同个算法，处理的数据量越大，它的时间复杂度也就越大。
>
> 约束Map输出：在Map Reduce框架里，Map之后的数据是要经过排序才执行Reduce操作的。想想归并排序算法的思想，对小文件进行排序，然后将小文件归并成大文件的思想，然后就会懂这点了

#### 3.为什么不推荐Map任务的切片大小设置为小于块的大小？

> 在Hadoop MapReduce中，"切片"（Split）是输入数据的逻辑分区，它通常等于HDFS中一个块的大小。
>
> 你可以设置切片大小，Hadoop框架提供了对切片大小进行配置的选项。对于每个切片，MapReduce框架会生成一个Map任务进行处理。
>
> **通常，我们会设置切片大小等于或接近块的大小。这样可以使得大部分Map任务在本地节点上处理本地数据，这被称为数据本地性（Data Locality）**，从而提高处理速度。
>
> 理论上，你可以设置切片大小小于块的大小，但是这通常并不推荐。因为如果一个块对应多个切片，则会**生成多个Map任务去处理这个块**，这可能导致更多的任务调度开销，并且可能导致数据本地性的利用率下降，**因为一个块只能在一个节点上存储**，而如果有多个Map任务需要处理这个块，那么就有可能需要跨节点传输数据。
>
> 因此，通常我们会设置切片大小等于或接近块的大小，以充分利用数据本地性，并减少任务调度开销。在某些特殊情况下，例如当处理的文件非常小的时候，可能会将切片大小设置为小于块的大小。