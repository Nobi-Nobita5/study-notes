一、spark sql简介

能够将sparkSql查询与Spark程序无缝混合，允许使用sql或DataFrame API对结构化数据进行查询

二、DataFrame & DataSet

1. DataFrame

   为了支持结构化数据的处理，spark sql提供了新的数据结构dataFrame。DataFrame是一个具有列名组成的数据集。

2. DataFrame 对比 RDDs

   一个面向结构化数据、一个面向非结构化数据

3. DataSet

   Dataset 也是分布式的数据集合，集成了RDD和DataFrame的优点，具备强类型的特点，同时支持Lambda函数，但只能在Scala和Java语言中使用。Spark提供了结构化的 API(Structured API) 将 DataFrame 和 Dataset 的 API 融合到一起。即一套标准的API完成对两者的操作。

   > DataFrame 被标记为 Untyped API，而 DataSet 被标记为 Typed API

4. 静态类型与运行时类型安全

   SparkSQL查询语句的报错在运行时才会被发现，DataFrame 和 Dataset则在编译时期就可以发现错误。

​		DataFrame 和 Dataset的区别在于：

​			在 DataFrame 中，调用了API之外的函数，编译器就会报错，但如果使用了一个不存在的字段名字，编译器依然无法发现。而			Dataset 的 API 都是用Lambda函数和JVM类型对象表示的，所有不匹配的类型参数在编译时就会被发现。

   >因为DataFrame 是 Untyped 的，所以在编译时期无法对列名进行推断和错误判断‘

5.  Untyped & Typed

   DataFrame API 被标记为 `Untyped API`，而 DataSet API 被标记为 `Typed API`。

   DataFrame确实有明确的Schema结构，即列名，列类型都是确定的，但这些信息完全由Spark来维护，Spark只会在运行时检查这些类型和指定类型是否一致；相对而言，DataSet 是 `Typed` 的，即强类型。如下面代码，DataSet 的类型由 Case Class(Scala) 或者 Java Bean(Java) 来明确指定的，在这里即每一行数据代表一个 `Person`。

   ~~~scala
   case class Person(name: String, age: Long)
   val dataSet: Dataset[Person] = spark.read.json("people.json").as[Person]
   ~~~

   DataSet 信息由JVM来保证正确性，所以字段名错误在编译时候就会被发现。

三、DataFrame & DataSet & RDDs 总结

* RDDs适合非结构化数据的处理，而DataFrame & DataSet更适合结构化数据和半结构化数据的处理；
* DataFrame & DataSet可以通过统一的Structured API进行访问，而RDDs更适合函数式变成的场景；
* 相比于DataFrame 而言，DataSet 是强类型的 (Typed)，有更为严格的静态类型检查；
* DataSets、DataFrames、SQL 的底层都依赖了RDDs API，并对外提供结构化的访问接口。

四、Spark SQL的运行原理

1. 逻辑计划
2. 物理计划