> SparkCore文档

#### 一、什么是RDD

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可以并行计算的集合。

#### 二、核心属性

1. 分区列表
2. 分区计算函数
3. RDD之间的依赖关系
4. 分区器
5. 首选位置

#### 三、执行原理

从计算的角度来看，数据处理需要**计算资源（内存 & CPU）和计算模型（逻辑）**

在yarn环境中，RDD的工作原理：

1. 启动yarn集群环境
2. Spark通过申请资源创建**调度节点**和**计算节点**
3. Spark框架根据需求将**计算逻辑**根据**分区**划分成**不同任务**
4. 调度节点将任务根据计算节点状态发送到对应的计算节点进行计算 

可以看出RDD主要用于封装逻辑，生成**Task**发送给**Executor**节点执行计算。

#### 四、创建RDD

1. 由现有集合创建
2. 引用外部存储系统中的数据集创建
3.  textFile & wholeTextFiles

#### 五、操作RDD

​	RDD支持两种类型的操作：transformations（转换，从现有数据集创建新数据集）和actions（在数据集上运行计算后将值返回到驱动程序）。RDD中的所有转换都是惰性的，他们只是记住这些转换操作，但不会立即执行，只有遇到action操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。

~~~
val list = List(1, 2, 3)
// map 是一个 transformations 操作，而 foreach 是一个 actions 操作
sc.parallelize(list).map(_ * 10).foreach(println)
// 输出： 10 20 30
~~~

#### 六、缓存RDD

​	Spark速度快的原因之一是支持缓存。缓存成功后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。

1. Spark 支持多种缓存级别 ：

2. 使用缓存：`persist`和`cache`

3. 移除缓存

   Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 `RDD.unpersist()` 方法进行手动删除。

#### 七、理解shuffle

	1. shuffle介绍
	1. shuffle的影响
	1. 导致shuffle的操作

#### 八、宽依赖和窄依赖

RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：

- **窄依赖 (narrow dependency)**：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；
- **宽依赖 (wide dependency)**：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。

#### 九、DAG(有向无环图)的生成

​	RDDs之间的依赖关系组成了DAG，如果一个RDD的部分或者全部计算丢失了，也可以重新计算。Spark主要根据依赖关系将不同的DAG划分为不同的计算阶段来生成计算任务。

- 对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；
- 对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。