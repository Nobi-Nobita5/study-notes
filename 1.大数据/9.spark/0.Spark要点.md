Spark面试干货总结：https://developer.aliyun.com/article/1366690?spm=5176.26934566.main.7.de556330ReR41s

## 一、Spark基础

### 1、Spark特点

Scala语言实现、能像操作本地集合对象一样轻松操作分布式数据集

1）运行速度快：拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是 Hadoop MapReduce 的 `10` 倍以上，如果数据从内存中读取，速度可以高达 `100` 倍。

2）易用：支持Scala、Java、Python

3）通用性：包含Spark Core、Spark Sql、Spark Streaming、MLLib和GraphX

4）随处运行：能够以Mesos、YARN和自身携带的Standalone作为资源调度器管理job，来完成Spark应用程序的计算

### 2、Spark生态圈包含哪些组建

1）Spark Core：Spark 的核心模块，包含 RDD、任务调度、内存管理、错误恢复、与存储系统交互等功能。

2）Spark Sql：主要用于进行结构化数据的处理。它提供的最核心的编程抽象就是 DataFrame，将其作为分布式 SQL 查询引擎，通过将 Spark SQL 转化为 RDD 来执行各种操作。

3）Spark Streaming：Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API。

4）Spark MLLib：提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能

5）GraphX（图计算）：Spark 中用于图计算的 API，性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。

6）Structured Streaming：处理结构化流，统一了实时和离线API

### 3、Spark提交方式、部署模式

1、local 本地模式。氛围local单线程和local-cluster多线程

2、standalone独立集群模式。

3、Spark on Yarn 集群模式。运行在 Yarn集群智商，由Yarn负责资源管理，Soark负责任务调度和计算

Spark on yarn根据Driver在集群中的位置氛围两种模式：

**yarn-client**：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出
**yarn-cluster**：Driver程序运行在由RM启动的 AppMaster中，适用于生产环境

### 4、Spark shuffle为什么比MapReduce shuffle快

1、Spark基于内存计算，MR基于磁盘

* Spark优先内存计算，仅在内存不足时才写磁盘
* MR输出必须写磁盘（本地中间文件），Reduce再从磁盘读取
* 优势：Spark避免了大量磁盘读写操作，减少了I/O开销

2、Pipeline机制提高效率

* Spark允许多个转换操作（如map--fliter--reduceBykey）在一个任务流水线中执行，减少中间结果落盘
* MR每个stage（Map和Reduce）都要完全执行完并写磁盘后，才能进入下一个stage

3、Spark有高效的调度算法，是基于DAG形成一系列的有向无环图

* Spark使用DAGScheduler + TaskScheduler分离的方式，构建高效构建任务依赖图，并采用延迟执行机制
* MR需等待上一个阶段完成后再调度下一个

4、Spark shuffle机制更灵活

* **Spark** 提供多种 Shuffle 策略（如 Tungsten Sort Shuffle、Bypass Merge Shuffle），可根据数据规模和资源优化选择。
* **MapReduce** 使用固定的 Sort-Merge Shuffle，不够灵活。
* **优势**：Spark Shuffle 可以根据不同场景优化内存与网络传输效率。

---

## 二、Spark Core

### 5、什么是RDD

弹性分布式数据集，可以理解为一种数据结构，拥有多种不同的RDD算子，从物理存储上看，一个数据集可能被分为多个分区，各个分区都有可能存放在不同的存储节点上，而RDD则是在该数据集上的一个抽象，代表了整个数据集，但这个RDD并不会从物理上讲数据放在一起。有了RDD这个抽象，用户可以从一个入口方便的操作一个分布式数据集。

特点：弹性（容错能力）、不可变（所有操作会生成新的RDD）、可分区（RDD是分区存储的）、可并行计算

### 6、RDD五大属性

1、RDD 之间的依赖关系 ：一个 RDD 会依赖于其他多个 RDD。RDD 的每次转换都会生成一个新的 RDD，所以 RDD 之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark 可以通过这个依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算。(Spark 的容错机制)

2、一组分区 ：一组分片(Partition)/一个分区(Partition)列表，即是数据集的基本组成单位，标记数据在哪个区。

3、一个 Partition ：可选项，对于 KV 类型的 RDD 会有一个 Partitioner，即 RDD 的分区函数，默认为 HashPartitioner。

4、一个列表：可选项,存储存取每个 Partition 的优先位置(preferred location)。对于一个 HDFS 文件来说，这个列表保存的就是每个 Partition 所在的块的位置。

5、一个计算每个分区的函数 ：一个函数会被作用在每一个分区。Spark 中 RDD 的计算是以**partition**为单位的，**compute 函数会被作用到每个分区上**。

### 7、常用算子

两种操作，一种转换transformation操作、一种动作action操作

1）Transformation操作：

Map、MapPartitions、FlatMap、Filter、distinct、sortBy、union、reduceByKey、groupByKey、sortByKey、join

2）action操作

reduce、collect、count、save、take、aggregate、countByKey等。

### 8、Map、MapPartitions区别

map：每次对 RDD 中的每一个元素进行操作；

mapPartitions：每次对 RDD 中的每一个分区的迭代器进行操作；

**mapPartitions 的优点：**如在 map 过程中需要频繁创建额外的对象(例如将 rdd 中的数据通过 jdbc 写入数据库,map 需要为每个元素创建一个链接而 mapPartitions 为每个 partition 创建一个链接),则 mapPartitions 效率比 Map 高的多。SparkSql 或 DataFrame 默认会对程序进行 mapPartitions 的优化。

**mapPartitions 的缺点：**会造成内存溢出。举例，对于 100 万数据，一次传入一个 function 以后，可能一下子内存不够，但是又没有办法腾出内存空间来，可能就OOM，内存溢出。

### 9、reduceByKey 和 GroupByKey的区别

reduceByKey()会在shuffle之前对数据进行合并。有点类似在MR中的combiner。这样做的好处在于，在转换操作时就已经对数据进行了一次聚合操作，从而减小数据传输

| 特性           | `reduceByKey`                          | `groupByKey`                                      |
| -------------- | -------------------------------------- | ------------------------------------------------- |
| **聚合时机**   | **先在每个分区内聚合**，再跨节点合并   | **直接将所有值发送到对应 key 的节点**，再统一分组 |
| **网络传输量** | 较小：**提前本地聚合**，减少跨节点数据 | 较大：所有 value 原封不动传输                     |
| **性能**       | **更快、更节省资源**                   | 容易导致 OOM，性能较差                            |

### 10、reduceByKey、foldByKey、aggregateByKey、combineByKey 区别

| 操作               | 初始值要求 | 分区内操作       | 分区间操作   | 灵活性         | 典型用途             |
| ------------------ | ---------- | ---------------- | ------------ | -------------- | -------------------- |
| **reduceByKey**    | 无         | 同 reduce        | 同 reduce    | 低             | 按 key 规约（合并）  |
| **foldByKey**      | 有         | `zeroValue` 初始 | 使用相同函数 | 低             | 可加上初始值的规约   |
| **aggregateByKey** | 有         | 一个函数         | 另一个函数   | 高（两个函数） | 分区内外处理逻辑不同 |
| **combineByKey**   | 无         | 三个函数         | 最灵活       | **最高**       | 自定义复杂聚合逻辑   |

选择建议

| 目标                  | 用法             |
| --------------------- | ---------------- |
| 简单求和/计数         | `reduceByKey`    |
| 需要初始值 + 简单聚合 | `foldByKey`      |
| 分区内/间逻辑不同     | `aggregateByKey` |
| 复杂结构聚合          | `combineByKey`   |

### 11、RDD的宽窄依赖

一个作业从开始到结束的计算过程中产生了多个 RDD，RDD 之间是彼此相互依赖的，这种父子依赖的关系称之为“血统”。

* 如果父RDD的每个分区最多只能被子RDD的一个分区使用，称之为窄依赖（一对一）
* 如果父RDD的每个分区可以被子RDD的多个分区使用，称之为宽依赖（一对多）

作用与意义：

1）决定是否产生shuffle

- **窄依赖**：无 Shuffle，数据在节点内处理 → 快
- **宽依赖**：产生 Shuffle，需跨节点通信 → 慢

2）影响任务划分（Stage划分）

* Spark 会在 **宽依赖处切分 Stage**

* 窄依赖的操作可以合并为一个 Stage

3）容错机制差异

* 窄依赖：只需重新计算单个父分区
* 宽依赖：丢一个分区可能要重新计算整个父RDD

4）性能优化参考依据

* 尽量用 `reduceByKey`（可局部聚合）避免 `groupByKey`

* 减少宽依赖操作，优化 DAG 结构和 Shuffle 次数

### 12、RDD缺点

1、批量数据写入，不支持realtime粒度的写操作。

2、不支持增量迭代计算（flink支持），原因如下：

1）RDD是不可变的

* RDD 每次 transformation 都会生成一个新 RDD，不能修改已有的数据。
* 所以 **每一次迭代都要重新计算整个数据集**，哪怕只变化了一小部分。

2）无状态模型

- RDD 本身 **不保留中间状态**。
- 即使你想保存某些迭代变量，也必须手动持久化（如 `persist()`），这不是真正的状态存储。

3）没有 Delta（增量）机制

- RDD 每次迭代都是**全量计算**，对大数据图计算等场景效率低。
- 例如 PageRank，每轮迭代都重新对整个图计算，而不是只对“值发生变化”的节点处理。

### 13、什么是DAG

有向无环图，指的是数据转换执行的过程，有方向，无闭环（其实就是RDD的执行流程）

原始的RDD通过一系列的转换操作就形成了DAG有向无环图，任务执行时，可以按照DAG的描述，执行真正的计算。

**DAG 的边界**

- 开始:通过 SparkContext 创建的 RDD；
- 结束:触发 Action，一旦触发 Action 就形成了一个完整的 DAG。

### 14、Spark广播变量和累加器？

广播变量（只读）：将一个只读变量复制到每个 Executor 的内存中，**避免在每个 task 中反复传输副本**，减少网络开销。

累加器（只写）：用于在各个 Executor 中**对某个变量进行累计**操作（如计数、求和），结果会汇总回 Driver。

### 15、Spark任务执行流程简图

Driver 程序
   ↓
构建 RDD → 生成 DAG（逻辑执行图）
   ↓
DAGScheduler：划分 Stage（根据宽/窄依赖）
   ↓
TaskScheduler：将 Stage 划分成 Task
   ↓
Cluster Manager：启动 Executor，分发 Task
   ↓
Executor 执行 Task，返回结果

### 16、如何使用 Spark 实现 TopN 的获取（描述思路）

见Spark特征优化

---

## 三、Spark Streaming

1、Spark Streaming 如何实现精确一次消费?

**概念：**

- 精确一次消费（Exactly-once） 是指消息一定会被处理且只会被处理一次。不多不少就一次处理。
- 至少一次消费（at least once），主要是保证数据不会丢失，但有可能存在数据重复问题。
- 最多一次消费 （at most once），主要是保证数据不会重复，但有可能存在数据丢失问题。

如果同时解决了**数据丢失**和**数据重复**的问题，那么就实现了**精确一次消费**的语义了。

**解决方案：**

方案一：利用`关系型数据库的事务`进行处理

出现丢失或者重复的问题，核心就是偏移量的提交与数据的保存，不是原子性的。如果能做成要么数据保存和偏移量都成功，要么两个失败。那么就不会出现丢失或者重复了。

这样的话可以把存数据和偏移量放到一个事务里。这样就做到前面的成功，如果后面做失败了，就回滚前面那么就达成了原子性。

方案二：`手动提交偏移量+幂等性处理`

首先解决**数据丢失问题**，**办法就是要等数据保存成功后再提交偏移量**，所以就必须**手工来控制偏移量**的提交时机。

再解决**数据重复问题，**如果数据保存了，没等偏移量提交进程挂了，数据会被重复消费。怎么办？那就要**把数据的保存做成幂等性保存**。即同一批数据反复保存多次，数据不会翻倍，保存一次和保存一百次的效果是一样的。如果能做到这个，就达到了幂等性保存，就不用担心数据会重复了。

**难点**

话虽如此，在实际的开发中手动提交偏移量其实不难，难的是幂等性的保存，有的时候并不一定能保证。所以有的时候只能优先保证的数据不丢失。数据重复难以避免。即只保证了至少一次消费的语义。
